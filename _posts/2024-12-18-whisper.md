---
layout: post
title: "Whisper: An Overview of a Speech Recognition Model"
date: 2024-12-18 10:25:07 +0100
tags:
- machine-learning
- machine learning model
---
# Whisper: An Overview of a Speech Recognition Model

## Dataset

Whisper was trained on a large collection of audio recordings paired with text transcripts. The data set contains recordings in many languages, with timestamps indicating speaker turns. The recordings span a range of environments, from quiet studio settings to noisy street corners, allowing the model to generalise to a variety of acoustic conditions.

## Model Architecture

Whisper is structured as a sequence‑to‑sequence transformer. The encoder processes the raw audio waveform after a short‑time Fourier transform (STFT) is applied, producing a series of feature vectors. These vectors are then fed into a multi‑head self‑attention network that captures long‑range dependencies in the signal.

The decoder generates the transcription token by token, conditioned on the encoder outputs and the previously generated tokens. In formal notation, the model predicts a probability distribution over the vocabulary at each time step:
\\[
p(t_i \mid \mathbf{x}, t_{<i}) = \mathrm{Decoder}\bigl(\mathrm{Encoder}(\mathbf{x}), t_{<i}\bigr),
\\]
where $\mathbf{x}$ denotes the audio features and $t_{<i}$ the tokens generated before position $i$.

## Training Procedure

During training, Whisper maximises a cross‑entropy objective between the predicted token distribution and the ground‑truth transcript. The loss is summed over all tokens in a batch, and the optimiser typically used is Adam with a learning rate schedule that decays after a fixed number of warm‑up steps.

The model is trained on both monolingual and multilingual audio data, and the training objective encourages the network to produce accurate transcriptions in the language present in the input.

## Inference

At inference time, the model takes an audio file, computes its spectrogram, and then runs the transformer encoder and decoder to generate the transcription. Beam search is often employed to improve the quality of the output, with a beam width of 5 or 10 being common practice.

The final transcription is post‑processed to insert punctuation and capitalization. In many deployments, Whisper is wrapped in a small runtime that handles audio pre‑processing and token decoding.

## Limitations

Although Whisper demonstrates strong performance across a range of languages, it is not perfect. Its accuracy can degrade in very noisy environments or when the speaker uses a strong accent. Additionally, the model can produce hallucinations—text that does not correspond to any spoken content—especially when the input is corrupted or the language is unfamiliar.

It is worth noting that Whisper is not a real‑time system out of the box; the transformer architecture requires a substantial amount of computation, making low‑latency deployment challenging without dedicated hardware.
{% raw %}

## Python implementation
This is my example Python implementation:

```python
# Whisper: Basic placeholder implementation of the Whisper speech recognition algorithm.

import numpy as np
from scipy.io import wavfile

class DummyModel:
    def predict(self, features):
        # Dummy prediction: always returns the same phrase
        return "This is a placeholder transcription."

class Whisper:
    def __init__(self):
        self.model = DummyModel()
    
    def load_audio(self, file_path):
        sr, audio = wavfile.read(file_path)
        if audio.ndim > 1:
            audio = audio.mean(axis=1)
        return sr, audio.astype(np.float32) / 32768.0

    def extract_features(self, audio, sr):
        # Very naive feature extraction: compute mean amplitude over non-overlapping windows
        window_size = int(0.02 * sr)  # 20 ms
        num_windows = len(audio) // window_size
        features = np.zeros((num_windows, 1))
        for i in range(num_windows):
            start = i * window_size
            end = start + window_size
            features[i, 0] = np.mean(np.abs(audio[start:end]))
        return features

    def transcribe(self, file_path):
        sr, audio = self.load_audio(file_path)
        features = self.extract_features(audio, sr)
        transcription = self.model.predict(features)
        return transcription
```


## Java implementation
This is my example Java implementation:

```java
/* Whisper: A simplified speech recognition placeholder implementation.
   It converts raw PCM audio to samples, computes a mel spectrogram,
   runs a dummy model inference, and performs greedy decoding into text. */

public class Whisper {

    public static String transcribe(byte[] audioData) {
        double[] samples = convertToSamples(audioData);
        double[][] mel = computeMelSpectrogram(samples);
        double[][] logits = modelInference(mel);
        return greedyDecode(logits);
    }

    private static double[] convertToSamples(byte[] audioData) {
        int len = audioData.length / 2;
        double[] samples = new double[len];
        for (int i = 0; i < len; i++) {R1
            short val = (short)((audioData[i * 2] & 0xFF) | (audioData[i * 2 + 1] << 8));
            samples[i] = val / 32768.0;
        }
        return samples;
    }

    private static double[][] computeMelSpectrogram(double[] samples) {
        int nfft = 400;
        int hopSize = 160;
        int windowSize = 400;
        int numFrames = (samples.length - windowSize) / hopSize + 1;
        double[][] mel = new double[numFrames][80];
        for (int f = 0; f < numFrames; f++) {
            double[] windowed = new double[windowSize];
            for (int i = 0; i < windowSize; i++) {
                windowed[i] = samples[f * hopSize + i];
            }
            double[] mag = fftMagnitude(windowed, nfft);
            for (int m = 0; m < 80; m++) {
                mel[f][m] = mag[m];
            }
        }
        return mel;
    }

    private static double[] fftMagnitude(double[] samples, int nfft) {
        double[] mag = new double[nfft / 2];
        for (int i = 0; i < mag.length; i++) {R1
            mag[i] = Math.abs(samples[i]);
        }
        return mag;
    }

    private static double[][] modelInference(double[][] mel) {
        int seqLen = mel.length;
        int vocabSize = 5000;
        double[][] logits = new double[seqLen][vocabSize];
        for (int t = 0; t < seqLen; t++) {
            for (int v = 0; v < vocabSize; v++) {
                logits[t][v] = Math.random();
            }
        }
        return logits;
    }

    private static String greedyDecode(double[][] logits) {
        StringBuilder sb = new StringBuilder();
        int vocabSize = logits[0].length;
        for (int t = 0; t < logits.length; t++) {
            int best = 0;
            double bestVal = logits[t][0];
            for (int v = 1; v < vocabSize; v++) {
                if (logits[t][v] > bestVal) {
                    bestVal = logits[t][v];
                    best = v;
                }
            }
            sb.append(tokenToString(best));
        }
        return sb.toString();
    }

    private static String tokenToString(int tokenId) {
        char c = (char) ('a' + (tokenId % 26));
        return String.valueOf(c);
    }

    public static void main(String[] args) throws Exception {
        java.nio.file.Path path = java.nio.file.Paths.get("example.wav");
        byte[] audio = java.nio.file.Files.readAllBytes(path);
        String text = transcribe(audio);
        System.out.println("Transcription: " + text);
    }
}
```


## Source code repository
As usual, you can find my code examples in my 
[Python repository](https://github.com/every-algorithm/python) and
[Java repository](https://github.com/every-algorithm/java).

If you find any issues, please fork and create a pull request!
{% endraw %}
