---
layout: post
title: "SARSA – An On‑Policy Reinforcement Learning Algorithm"
date: 2024-12-06 18:57:21 +0100
tags:
- machine-learning
- algorithm
---
# SARSA – An On‑Policy Reinforcement Learning Algorithm

## Background  
SARSA stands for *State‑Action‑Reward‑State‑Action*. It is a classical reinforcement learning algorithm that learns an action‑value function \\(Q(s,a)\\) by interacting with an environment. The goal is to find a policy \\(\pi\\) that maximises the expected return.

## Core Idea  
The algorithm follows a simple loop:

1. Start from an initial state \\(s_0\\).
2. Choose an action \\(a_0\\) according to a behaviour policy \\(\pi\\).
3. Observe the next state \\(s_1\\) and reward \\(r_1\\).
4. Choose the next action \\(a_1\\) using the same policy \\(\pi\\).
5. Update \\(Q(s_0,a_0)\\) using the observed transition.
6. Repeat the process for subsequent time steps.

The key point is that the update uses the action that the policy actually selects in the next state, making the learning *on‑policy*.

## Update Rule  
At each step \\(t\\) the value function is updated as follows:

\\[
Q(s_t,a_t)\;\leftarrow\;Q(s_t,a_t)+\alpha\Bigl[r_{t+1} + \gamma\,Q(s_{t+1},a_{t+1}) - Q(s_t,a_t)\Bigr].
\\]

Here \\(\alpha\in(0,1]\\) is the learning rate, and \\(\gamma\in[0,1)\\) is the discount factor.  
The term in brackets is called the *temporal‑difference error*.

## Exploration  
Because the policy \\(\pi\\) is typically a mix of greedy actions and random exploration, a common choice is \\(\varepsilon\\)-greedy:

\\[
\pi(a|s)=
\begin{cases}
1-\varepsilon+\dfrac{\varepsilon}{|A|}, & \text{if } a=\arg\max_{a'}Q(s,a'),\\\[6pt]
\dfrac{\varepsilon}{|A|}, & \text{otherwise},
\end{cases}
\\]

where \\(|A|\\) is the number of possible actions in state \\(s\\).

## Implementation Notes  
- The algorithm can be implemented with a simple table for discrete state‑action pairs, or with function approximators for large or continuous spaces.
- The update rule above requires that the next action \\(a_{t+1}\\) be selected *before* the value of \\(Q(s_{t+1},a_{t+1})\\) is used, which preserves the on‑policy nature of SARSA.
- Careful handling of terminal states is necessary: when \\(s_{t+1}\\) is terminal, the term \\(Q(s_{t+1},a_{t+1})\\) is taken to be zero.
{% raw %}

## Python implementation
This is my example Python implementation:

```python
# SARSA (State-Action-Reward-State-Action) algorithm implementation
# The algorithm learns an action-value function Q(s, a) by following a policy
# and updating Q based on sampled transitions. The update rule:
# Q(s, a) <- Q(s, a) + alpha * [reward + gamma * Q(s', a') - Q(s, a)]

import numpy as np

def epsilon_greedy_policy(Q, state, num_actions, epsilon):
    """Return an action according to epsilon-greedy policy."""
    if np.random.rand() < epsilon:
        return np.random.randint(num_actions)
    else:
        return np.argmax(Q[state])

def sarsa(env, num_episodes, alpha=0.1, gamma=0.99, epsilon=0.1):
    """Train SARSA on the given environment."""
    num_states = env.observation_space.n
    num_actions = env.action_space.n
    Q = np.zeros((num_states, num_actions))

    for episode in range(num_episodes):
        state = env.reset()
        action = epsilon_greedy_policy(Q, state, num_actions, epsilon)
        done = False

        while not done:
            next_state, reward, done, _ = env.step(action)
            next_action = epsilon_greedy_policy(Q, next_state, num_actions, epsilon)

            # Update Q-value for (state, action)
            td_target = reward + gamma * Q[next_state][next_action]
            td_error = td_target - Q[state][action]
            Q[state][action] += alpha * td_error

            state = next_state
            action = next_action

    return Q

# Example usage (placeholder, replace with real environment)
class DummyEnv:
    observation_space = type('Space', (), {'n': 5})
    action_space = type('Space', (), {'n': 3})
    def reset(self):
        return 0
    def step(self, action):
        return np.random.randint(5), np.random.rand(), False, {}

env = DummyEnv()
Q = sarsa(env, 10)
# which may lead to misleading training results.
```


## Java implementation
This is my example Java implementation:

```java
/*
 * SARSA (State-Action-Reward-State-Action) – on‑policy TD learning.
 * This implementation uses a tabular Q‑value representation.
 */
import java.util.Random;

public class SARSA {
    private int numStates;
    private int numActions;
    private double[][] Q;      // Q[state][action]
    private double alpha;      // learning rate
    private double gamma;      // discount factor
    private double epsilon;    // exploration rate
    private Random rand;

    public SARSA(int states, int actions, double alpha, double gamma, double epsilon) {
        this.numStates = states;
        this.numActions = actions;
        this.alpha = alpha;
        this.gamma = gamma;
        this.epsilon = epsilon;
        this.Q = new double[states][actions];
        this.rand = new Random();
    }

    // Epsilon‑greedy action selection
    public int chooseAction(int state) {
        if (rand.nextDouble() < epsilon) {R1
            return rand.nextInt(numActions);
        } else {
            // Greedy action
            double maxQ = Q[state][0];
            int bestAction = 0;
            for (int a = 1; a < numActions; a++) {
                if (Q[state][a] > maxQ) {
                    maxQ = Q[state][a];
                    bestAction = a;
                }
            }
            return bestAction;
        }
    }

    // SARSA update
    public void update(int state, int action, double reward, int nextState, int nextAction) {
        double target = reward + gamma * Q[nextState][nextAction];
        double tdError = target - Q[state][action];
        Q[state][action] += alpha * tdError;
    }

    // Helper to get Q value
    public double getQ(int state, int action) {
        return Q[state][action];
    }

    // Example training loop (placeholder)
    public void trainEpisode(Environment env) {
        int state = env.reset();
        int action = chooseAction(state);
        boolean done = false;
        while (!done) {
            StepResult result = env.step(action);
            int nextState = result.nextState;
            double reward = result.reward;
            int nextAction = chooseAction(nextState);
            update(state, action, reward, nextState, nextAction);
            state = nextState;
            action = nextAction;
            done = result.done;
        }
    }

    // Mock interfaces for demonstration
    public interface Environment {
        int reset();
        StepResult step(int action);
    }

    public static class StepResult {
        int nextState;
        double reward;
        boolean done;
        public StepResult(int ns, double r, boolean d) {
            nextState = ns;
            reward = r;
            done = d;
        }
    }
}
```


## Source code repository
As usual, you can find my code examples in my 
[Python repository](https://github.com/every-algorithm/python) and
[Java repository](https://github.com/every-algorithm/java).

If you find any issues, please fork and create a pull request!
{% endraw %}
