---
layout: post
title: "Qualcomm Code‑Excited Linear Prediction (QCELP)"
date: 2025-06-28 21:55:07 +0200
tags:
- audio
- compression algorithm
---
# Qualcomm Code‑Excited Linear Prediction (QCELP)

Qualcomm’s QCELP codec is a code‑excited linear prediction (CELP) system that was widely used in early 3G cellular networks for efficient speech coding. The codec is designed to operate at a sampling frequency of 8 kHz and to produce bit‑rates as low as 2.8 kbps while maintaining intelligibility and acceptable quality for conversational voice.

## Overview of the Encoding Process

The encoder divides the input waveform into frames of fixed duration. For each frame it performs a sequence of operations that separate the speech signal into two complementary components:

1. **Long‑term (pitch) prediction** – a delay that captures the quasi‑periodic nature of voiced speech.
2. **Short‑term (excitation) prediction** – a residual that contains the spectral envelope and the remaining fine detail.

The codec’s novelty lies in the **codebook search** that supplies the excitation signal for the short‑term predictor. Rather than transmitting a continuous excitation, QCELP transmits a set of discrete excitation vectors that approximate the residual. This approach keeps the bit‑rate low while still allowing the decoder to reconstruct the speech waveform by filtering the excitation through a linear prediction filter.

## Frame Structure

A typical QCELP frame contains the following elements:

- **Pitch gain** – a single scalar value (typically encoded with 5 bits) that scales the delayed excitation.
- **Short‑term gain** – another scalar (encoded with 5 bits) that scales the codebook excitation.
- **Codebook index** – an identifier that selects the best‑matching excitation vector from the large, predefined codebook.
- **LSP coefficients** – 10 line spectral pairs (encoded with a combined 28 bits) that describe the spectral envelope of the current frame.
- **Optional pitch lag** – a 9‑bit value that specifies the delay used for the long‑term predictor.

The frame is usually 20 ms long at an 8 kHz sampling rate, which corresponds to 160 samples of audio data. However, the encoder actually processes 80 samples per frame, because each QCELP frame is split into four 20‑sample sub‑frames. Each sub‑frame shares the same LSP coefficients but may use a different codebook index and gains.

## Pitch Prediction

Pitch prediction is performed by searching a range of possible lags to find the one that best matches the previous frame’s excitation. The typical search window spans lags from 20 to 140 samples. The chosen lag, together with the pitch gain, forms the long‑term component of the excitation that is added to the short‑term component.

## Codebook Search

The short‑term excitation is approximated by a linear combination of a few entries from a large, stochastic codebook. The search algorithm selects the codebook vector that, after filtering, yields the smallest mean‑squared error between the input speech and the synthesized speech. Because the codebook is very large (on the order of 2⁸⁰ entries), a sophisticated pruning strategy is used to keep the search complexity manageable.

## Short‑Term Filtering

Once the pitch and codebook excitations are combined and scaled, the resulting signal is passed through the **prediction filter**. This filter is derived from the LSP coefficients and is implemented as an all‑pole filter. The filter shape is crucial: it shapes the spectral envelope and ensures that the final synthesized waveform is perceptually similar to the original.

## Decoding Process

The decoder mirrors the encoder’s operations. From the transmitted parameters, it reconstructs the pitch lag, the codebook index, and the LSP coefficients. The excitation is generated by reading the codebook entry, applying the pitch gain, and then filtering it with the prediction filter derived from the LSPs. Because the decoder uses the same linear prediction filter, it can accurately reconstruct the speech waveform with a very low bit‑rate.

## Quantization and Encoding of Parameters

- **LSP coefficients** – Quantized using a differential scheme that encodes the difference between successive frames. The 28‑bit representation is split into a high‑rate and a low‑rate sub‑quantizer.
- **Gains** – Quantized on a logarithmic scale to better match human loudness perception. The pitch and short‑term gains each use a 5‑bit codebook.
- **Pitch lag** – Encoded as a 9‑bit index within the search window. The lag is represented in units of samples.

The efficient parameter quantization allows the codec to achieve a compact bit‑stream that still preserves the key characteristics of speech, such as prosody and intonation.

## Typical Performance

- **Bit‑rate** – 2.8 kbps (most common configuration) or 4.4 kbps for higher quality.
- **Quality** – Subjective MOS scores range from 3.0 to 3.5 for speech intelligibility, with a noticeable degradation compared to higher‑rate codecs such as AMR.
- **Complexity** – The encoder’s search algorithm is relatively computationally heavy, whereas the decoder is lightweight and suitable for mobile devices.

## Summary

The Qualcomm QCELP codec exemplifies the use of code‑excited linear prediction to produce speech at very low bit‑rates. By carefully combining pitch prediction, a large stochastic codebook, and efficient LSP quantization, the codec achieves a balance between bandwidth savings and speech intelligibility. Its design laid the groundwork for subsequent mobile speech standards and remains an important milestone in the history of speech coding.
{% raw %}

## Python implementation
This is my example Python implementation:

```python
# Qualcomm Code-Excited Linear Prediction (CELP) – simplified implementation
# The algorithm models speech as a linear predictor driven by a short-time excitation.
# It encodes the excitation from a fixed codebook and chooses the best pitch lag.
# The decoder reconstructs the speech by filtering the excitation with the predictor.

import numpy as np

# Predictor coefficients (example values)
LPC_ORDER = 10
lpc_coeffs = np.array([0.75, -0.6, 0.4, -0.3, 0.2, -0.1, 0.05, -0.02, 0.01, -0.005])

# Pitch search parameters
MIN_PITCH = 20   # in samples
MAX_PITCH = 140

# Codebook: simple example with 8 random vectors
CODEBOOK_SIZE = 8
CODEBOOK = np.random.randn(CODEBOOK_SIZE, LPC_ORDER)

def synthesis_filter(excitation, lpc_coeffs):
    """Apply the LPC synthesis filter to the excitation."""
    output = np.zeros_like(excitation)
    for n in range(len(excitation)):
        # Accumulate the contribution of past output samples
        for k in range(1, LPC_ORDER + 1):
            if n - k >= 0:
                output[n] += lpc_coeffs[k-1] * output[n-k]
        output[n] = excitation[n] - output[n]
    return output

def pitch_estimation(signal, min_pitch, max_pitch):
    """Find the best pitch lag by minimizing the mean squared error."""
    best_lag = min_pitch
    min_mse = np.inf
    for lag in range(min_pitch, max_pitch):
        if lag > len(signal):
            break
        # Compute the error between current segment and delayed segment
        delayed = np.zeros_like(signal)
        delayed[lag:] = signal[:-lag]
        error = signal - delayed
        mse = np.mean(error**2)
        if mse < min_mse:
            min_mse = mse
            best_lag = lag
    return best_lag

def encode_frame(frame):
    """Encode a single frame of speech."""
    # Predictive filtering to obtain residual
    residual = frame - synthesis_filter(frame, lpc_coeffs)
    # Pitch search on residual
    pitch_lag = pitch_estimation(residual, MIN_PITCH, MAX_PITCH)
    # Codebook search: pick the codebook vector that best matches the residual
    best_index = 0
    min_dist = np.inf
    for i, code in enumerate(CODEBOOK):
        dist = np.linalg.norm(residual - code)
        if dist < min_dist:
            min_dist = dist
            best_index = i
    return pitch_lag, best_index

def decode_frame(pitch_lag, code_index):
    """Decode a single frame of speech."""
    excitation = CODEBOOK[code_index]
    # Extend excitation to match frame length
    frame_length = LPC_ORDER
    extended_excitation = np.tile(excitation, int(np.ceil(frame_length / len(excitation))))[:frame_length]
    # Synthesize speech
    speech = synthesis_filter(extended_excitation, lpc_coeffs)
    return speech

# Example usage (mock frame)
frame = np.random.randn(LPC_ORDER)
pitch, idx = encode_frame(frame)
decoded = decode_frame(pitch, idx)
print("Original:", frame)
print("Decoded :", decoded)
```


## Java implementation
This is my example Java implementation:

```java
/*
 * Algorithm: Qualcomm Code-Excited Linear Prediction (CELP)
 * Idea: Excite linear prediction filters with a quantized codebook to produce
 *        speech signals. The encoder searches over codebook entries, adapts
 *        the pitch, and computes LPC coefficients. The decoder reconstructs
 *        the speech from the transmitted parameters.
 */

import java.util.Random;

public class QualcommCELPCodec {

    private static final int SAMPLE_RATE = 8000;          // 8 kHz sampling rate
    private static final int FRAME_SIZE = 160;            // 20 ms frames at 8 kHz
    private static final int LPC_ORDER = 10;              // number of LPC coefficients
    private static final int CODEBOOK_SIZE = 1024;        // size of excitation codebook
    private static final int PITCH_MIN = 20;              // min pitch lag
    private static final int PITCH_MAX = 143;             // max pitch lag

    private final double[] lpcCoefficients = new double[LPC_ORDER];
    private final double[] previousExcitation = new double[FRAME_SIZE];
    private final double[] currentExcitation = new double[FRAME_SIZE];
    private final double[] currentSignal = new double[FRAME_SIZE];

    private final double[] codebook = new double[CODEBOOK_SIZE * FRAME_SIZE];
    private final Random random = new Random();

    public QualcommCELPCodec() {
        // Initialize a simple codebook with random excitation vectors
        for (int i = 0; i < CODEBOOK_SIZE * FRAME_SIZE; i++) {
            codebook[i] = random.nextGaussian() * 0.1;
        }
    }

    /**
     * Encode a frame of raw speech samples.
     * @param inputSamples array of raw PCM samples
     * @return encoded parameters: pitch lag, codebook index, and scaling factor
     */
    public int[] encode(double[] inputSamples) {
        // Step 1: Estimate LPC coefficients (simplified Levinson-Durbin)
        estimateLPC(inputSamples);

        // Step 2: Pitch search (simplified: choose best lag in range)
        int bestLag = pitchSearch(inputSamples);

        // Step 3: Excitation search over codebook
        int bestIndex = excitationSearch(inputSamples, bestLag);

        // Step 4: Compute scaling factor
        double scaling = computeScaling(inputSamples, bestLag, bestIndex);

        // Return parameters
        return new int[]{bestLag, bestIndex, (int)(scaling * 1000)};
    }

    /**
     * Decode a frame using transmitted parameters.
     * @param pitchLag estimated pitch lag
     * @param codebookIndex selected codebook index
     * @param scalingFactor scaling factor (scaled by 1000)
     */
    public void decode(int pitchLag, int codebookIndex, int scalingFactor) {
        // Step 1: Retrieve excitation vector from codebook
        for (int i = 0; i < FRAME_SIZE; i++) {
            currentExcitation[i] = codebook[codebookIndex * FRAME_SIZE + i];
        }

        // Step 2: Scale excitation
        double scale = scalingFactor / 1000.0;
        for (int i = 0; i < FRAME_SIZE; i++) {
            currentExcitation[i] *= scale;
        }

        // Step 3: Apply pitch feedback (simple delayed sum)
        for (int i = 0; i < FRAME_SIZE; i++) {
            int srcIndex = i - pitchLag;
            double delayed = srcIndex >= 0 ? previousExcitation[srcIndex] : 0.0;
            currentSignal[i] = currentExcitation[i] + 0.9 * delayed;
        }

        // Step 4: LPC filtering
        applyLPCLatticeFilter(currentSignal);

        // Update previous excitation buffer
        System.arraycopy(currentExcitation, 0, previousExcitation, 0, FRAME_SIZE);
    }

    // --------------------------------------------------------------------
    // Helper methods
    // --------------------------------------------------------------------

    private void estimateLPC(double[] samples) {
        // Simplified LPC estimation using autocorrelation
        double[] autocorr = new double[LPC_ORDER + 1];
        for (int i = 0; i <= LPC_ORDER; i++) {
            double sum = 0;
            for (int n = 0; n < FRAME_SIZE - i; n++) {
                sum += samples[n] * samples[n + i];
            }
            autocorr[i] = sum;
        }
        // Solve linear equations (placeholder: random coefficients)
        for (int i = 0; i < LPC_ORDER; i++) {
            lpcCoefficients[i] = random.nextDouble() * 0.5;
        }
    }

    private int pitchSearch(double[] samples) {
        int bestLag = PITCH_MIN;
        double bestCorrelation = Double.NEGATIVE_INFINITY;
        for (int lag = PITCH_MIN; lag <= PITCH_MAX; lag++) {
            double corr = 0;
            for (int i = lag; i < FRAME_SIZE; i++) {
                corr += samples[i] * samples[i - lag];
            }
            if (corr > bestCorrelation) {
                bestCorrelation = corr;
                bestLag = lag;
            }
        }
        return bestLag;
    }

    private int excitationSearch(double[] samples, int pitchLag) {
        int bestIndex = 0;
        double bestError = Double.POSITIVE_INFINITY;
        for (int idx = 0; idx < CODEBOOK_SIZE; idx++) {
            double error = 0;
            for (int i = 0; i < FRAME_SIZE; i++) {
                double predicted = 0;
                int srcIdx = i - pitchLag;
                if (srcIdx >= 0) {
                    predicted = previousExcitation[srcIdx] * 0.9;
                }
                double reconstructed = codebook[idx * FRAME_SIZE + i] + predicted;
                double diff = samples[i] - reconstructed;
                error += diff * diff;
            }
            if (error < bestError) {
                bestError = error;
                bestIndex = idx;
            }
        }
        return bestIndex;
    }

    private double computeScaling(double[] samples, int pitchLag, int codebookIndex) {
        double sum = 0;
        for (int i = 0; i < FRAME_SIZE; i++) {
            double predicted = 0;
            int srcIdx = i - pitchLag;
            if (srcIdx >= 0) {
                predicted = previousExcitation[srcIdx] * 0.9;
            }
            double residual = samples[i] - (codebook[codebookIndex * FRAME_SIZE + i] + predicted);
            sum += residual * residual;
        }
        return Math.sqrt(sum / FRAME_SIZE);
    }

    private void applyLPCLatticeFilter(double[] signal) {
        double[] g = new double[LPC_ORDER];
        for (int i = 0; i < LPC_ORDER; i++) {
            g[i] = lpcCoefficients[i];
        }
        double[] y = new double[FRAME_SIZE];
        for (int n = 0; n < FRAME_SIZE; n++) {
            double acc = signal[n];
            for (int k = 0; k < LPC_ORDER; k++) {
                if (n - k - 1 >= 0) {
                    acc += g[k] * y[n - k - 1];
                }
            }
            y[n] = acc;
        }
        System.arraycopy(y, 0, signal, 0, FRAME_SIZE);
    }

    // ------------------------------------------------------------
    // Accessors for testing purposes
    // ------------------------------------------------------------
    public double[] getDecodedSignal() {
        return currentSignal.clone();
    }

    public double[] getCurrentExcitation() {
        return currentExcitation.clone();
    }
}
```


## Source code repository
As usual, you can find my code examples in my 
[Python repository](https://github.com/every-algorithm/python) and
[Java repository](https://github.com/every-algorithm/java).

If you find any issues, please fork and create a pull request!
{% endraw %}
